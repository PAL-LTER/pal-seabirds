# Palmer LTER Seabirds Archive

This repository includes the raw datafiles and code needed to aggregate the long-term Seabird datasets for the [Palmer LTER](http://pallter.marine.rutgers.edu) project. This repository includes both the cruise and Palmer Station area datafiles.  Following the 2020 field season, data is collected and managed by Megan Cimino (University of California at Santa Cruz and NOAA).  Before that, seabird data was collected by William Fraser (Polar Oceans Research Group).

The processed datasets are archived in Environmental Data Initiativeâ€™s data repository (see this link to search for [PAL penguin datasets](http://portal.edirepository.org:80/nis/simpleSearch?defType=edismax&q=subject:%22penguin%22&fq=-scope:ecotrends&fq=-scope:lter-landsat*&fq=scope:(knb-lter-pal)&fl=id,packageid,title,author,organization,pubdate,coordinates&debug=false)).

This repo is currently managed by Sage Lichtenwalner, PAL Information Manager, Rutgers University

## Repository Contents
Here are the key directories and files in this repo:
* [formatted](formatted) - This directory includes the reformatted data files, generated by the scripts found in the `original` directory.
* [original](original) - This directory includes the original raw data files as provided by the science team, broken up into each year/field season.  It also includes scripts to convert the raw files into standard CSV files for each dataset.  In general, cruise and station datasets are processed separately, so there are generally two scripts for each year.  The older Fraser datasets are provided as single Excel files.
* [merge_cruise.py](merge_cruise.py) - This script merges the cruise datasets.
* [merge_files.py](merge_files.py) - This script merges the station datasets.

## Processing Steps
This dataset is typically updated every year, after the austral summer field season.  Use these steps to update the datasets.

1. Add the original data files (provided by the field team) to the `original` directory, into a new directory for that year.
2. If necessary, create a new script to reformat the original files into the common CSV format for each datasets.  (See the formatted directory for the format to match for each dataset.)
3. Move the reformatted files to the appropriate subdirectory in the `formatted` directory.
4. Run the appropriate merge script.
  * You can specify `--dataset all` to process all datasets handled by the script, or you can specify a single dataset to process.
  * You can also use the `--suffix` to customize the file suffix.  By default *_merged.csv* will be used.  We recommend specifying the year range, e.g. `--suffix 1991-2024` for the final files. 
5. Optional: Update the `compare_station` script to compare the latest datasets with the previously archived versions on EDI.  (Make sure the input and output filenames are correct.)  Review the output to make sure the updates are correct.
6. If desired, move the new files into a YEAR subdirectory.
7. Use ezEML to update the metadata for the new datasets for archiving.
